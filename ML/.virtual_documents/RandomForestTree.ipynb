import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import RandomizedSearchCV, train_test_split
from sklearn.preprocessing import StandardScaler


from sklearn.datasets import load_breast_cancer


lbc = load_breast_cancer()
bc = pd.DataFrame(lbc.data, columns = lbc.feature_names)
bc['target'] = lbc.target

train_bc, test_bc = train_test_split(bc, test_size=0.2, random_state=123)


rf = RandomForestClassifier(oob_score=True)


n_estimators = np.arange(5, 20)
criterion = ['gini', 'entropy']
min_samples_split = [30, 40, 50, 60]
min_samples_leaf = [20, 25, 30, 35]
max_features = ['sqrt', 'log2', 0.2, 0.45, 0.7]


param_rf = {'n_estimators': n_estimators,
           'criterion': criterion,
           'min_samples_split': min_samples_split,
           'min_samples_leaf': min_samples_leaf,
           'max_features': max_features}


rf_random = RandomizedSearchCV(estimator = rf, param_distributions=param_rf, n_iter=25, n_jobs=-1)


rf_random.fit(train_bc.drop(columns='target'), train_bc.target)


rf_random.best_estimator_


rf_random.predict(test_bc.drop(columns='target'))


from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer


housing = pd.read_excel('housing.xlsx')


housing.head()


#Check for missing values
housing.isna().sum()





num_cols = housing.select_dtypes(exclude='O').columns
cat_cols = housing.select_dtypes(include='O').columns


cat_cols


np.shape(num_cols)


si = SimpleImputer(strategy='median')


si.fit_transform(housing[num_cols])


ss=StandardScaler()


ss.fit_transform(si.fit_transform(housing[num_cols]))


num_pipeline = Pipeline([('imputer', SimpleImputer(strategy='median')), ('std_scaler', StandardScaler())])


num_pipeline.fit_transform(housing[num_cols])


from sklearn.preprocessing import OneHotEncoder


housing[num_cols].head()


preprocessing = ColumnTransformer([('num', num_pipeline, num_cols), ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)])


np.shape(preprocessing.fit_transform(housing))


pd.DataFrame(preprocessing.fit_transform(housing))


oe = OneHotEncoder()


oe.fit_transform(housing[['ocean_proximity']],)


oe.feature_names_in_


oe.categories_


from xgboost import XGBRegressor


xgb = XGBRegressor()


final_pipeline = Pipeline([('preprocess', preprocessing), ('xgb', xgb)])


final_pipeline


#xgb__
# gbm_param = {
#     'xgb__n_estimators':[40, 50, 60],
#     'xgb__max_depth':[4,5 ,7],
#     'xgb__learning_rate': [0.4, 0.2, 0.5, 0.8],
#     'xgb__colsample_bytree':[0.01, 0.02, 0.03, 0.04, 0.05, 0.08]
    
# }
gbm_param = {
    'xgb__n_estimators': [40,50,60],
    'xgb__max_depth':[4,6,8],
    'xgb__learning_rate':[.4,.2,.5,.8],
    'xgb__colsample_bytree':[.01,.02,.03,.4,.5,.8]
}





from sklearn.model_selection import RandomizedSearchCV


housing.drop(columns='median_house_value')


housing.median_house_value


final_pipeline


final_pipeline.fit(housing, housing.median_house_value)


xgb_rscv = RandomizedSearchCV(estimator=final_pipeline, scoring='neg_mean_squared_error', param_distributions=gbm_param, n_iter=20, n_jobs=-1)


xgb_rscv.fit(housing, housing.median_house_value)


xgb_rscv
