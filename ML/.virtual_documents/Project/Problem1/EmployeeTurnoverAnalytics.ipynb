


import pandas as pd
from sklearn.preprocessing import OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler


data_raw = pd.read_csv('HR_comma_sep.csv')


data_raw.head()


data_raw.isna().sum()
num_cols = data_raw.select_dtypes(exclude='O')
cat_cols = data_raw.select_dtypes(include='O')








import matplotlib.pyplot as plt
import seaborn as sns

correlation = data_raw.drop(columns = ['sales', 'salary', ]).corr()

axis_corr = sns.heatmap(
correlation,
vmin=-1, vmax=1, center=0,
cmap=sns.diverging_palette(50, 500, n=500),
square=True
)





import matplotlib.pyplot as plt
import seaborn as sns

plt.hist(data_raw.satisfaction_level)
plt.xlabel('satisfaction_level')
plt.show()

plt.hist(data_raw.last_evaluation)
plt.xlabel('last_evaluation')
plt.show()

plt.hist(data_raw.average_montly_hours)
plt.xlabel('average_montly_hours')
plt.show()


import matplotlib.pyplot as plt
import seaborn as sns
data_raw['left'] = data_raw['left'].astype('str')
sns.countplot(data_raw, x='number_project', hue='left')
plt.title('Employee Project Count for Employees Who Left and Stayed')
plt.xlabel('Number of Projects')
plt.ylabel('Count of Employees')
plt.legend(title='Left', labels=['Stayed', 'Left'])
plt.show()





from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
ss =StandardScaler()


cols = ['satisfaction_level', 'last_evaluation']


data_raw['left'] 


data_people_left = data_raw[data_raw['left'] == '1']
data_for_cluster = data_people_left[cols]


data_for_cluster


n_clusters = 3
kmn = KMeans(n_clusters=n_clusters)


data_for_cluster_std = ss.fit_transform(data_for_cluster)
indices = kmn.fit_predict(data_for_cluster_std)


labels = kmn.labels_


# Plot the clusters in 3D
fig = plt.figure(figsize=(10, 7))
ax = fig.add_subplot(111)

# Plot each cluster with a different color
colors = ['r', 'g', 'b']
for i in range(n_clusters):
    ax.scatter(data_for_cluster_std[indices == i, 0], data_for_cluster_std[indices == i, 1], c=colors[i], label=f'Cluster {i + 1}')

# Plot the cluster centers
centers = kmn.cluster_centers_
ax.scatter(centers[:, 0], centers[:, 1], c='yellow', s=300, alpha=0.6, label='Centroids')

ax.set_xlabel('satisfaction_level')
ax.set_ylabel('last_evaluation')

ax.legend()
plt.show()


plt.hist(indices)











from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import SGDClassifier, SGDRegressor
from sklearn.compose import ColumnTransformer
from sklearn.metrics import classification_report
from sklearn.model_selection import GridSearchCV


#Create all the processing instances
skf = StratifiedKFold(n_splits=5)
ss = StandardScaler()
logR_mdl = SGDClassifier(loss='log_loss')


# data_raw.left = data_raw.left.astype('int')


type(data_raw.left[0])


num_cols = data_raw.select_dtypes(exclude='O').columns
cat_cols = data_raw.select_dtypes(include='O').columns
cat_cols = [cols for cols in cat_cols if cols != 'left']
num_cols = [cols for cols in num_cols if cols != 'left']



num_pipeline = Pipeline([('scaler',StandardScaler())])
cat_pipeline = Pipeline([('oe', OneHotEncoder())])

preprocessing = ColumnTransformer([('num_ppln', num_pipeline, num_cols), ('cat_ppln', cat_pipeline, cat_cols)])
master_pipeline = Pipeline([('preprocessing', preprocessing), ('model', logR_mdl)])


X_train, X_test = train_test_split(data_raw, test_size = 0.3, random_state=123)


gbm_param = {
    'model__penalty':['l2', 'l1', 'elasticnet'],
    'model__alpha' : [0.01, 1, 10, 100, 1000],
    'model__l1_ratio': [0, 1, 0.5, 0.2, 0.8],
    'model__learning_rate' : ['constant', 'adaptive'],
    'model__eta0' :[0.001, 0.01, 0.1, 0.02, 0.8, 0.08, 1, 10, 100]
}


logisticReg_gscv = GridSearchCV(estimator=master_pipeline, scoring='neg_mean_squared_error', param_grid=gbm_param, n_jobs=-1, refit=True)


logisticReg_gscv.fit(X_train, X_train.left.astype('int'))


pd.DataFrame(logisticReg_gscv.cv_results_)


logisticReg_gscv.best_params_


print(classification_report(X_test.left.astype('int'), logisticReg_gscv.predict(X_test.drop(columns=['left']))))


from sklearn.ensemble import RandomForestClassifier
import numpy as np


param_rf = {'model__n_estimators':  np.arange(5, 20),
           'model__criterion': ['gini', 'entropy'],
           'model__min_samples_split': [30, 40, 50, 60],
           'model__min_samples_leaf': [20, 25, 30, 35],
           'model__max_features': ['sqrt', 'log2', 0.2, 0.45, 0.7]}


num_pipeline_rf = Pipeline([('scaler',StandardScaler())])
cat_pipeline_rf = Pipeline([('oe', OneHotEncoder())])

rf_mdl = RandomForestClassifier()

preprocessing_rf = ColumnTransformer([('num_ppln', num_pipeline_rf, num_cols), ('cat_ppln', cat_pipeline_rf, cat_cols)])
master_pipeline_rf = Pipeline([('preprocessing', preprocessing_rf), ('model', rf_mdl)])



rf_gscv = GridSearchCV(estimator=master_pipeline_rf, scoring='neg_mean_squared_error', param_grid=param_rf, n_jobs=-1, refit=True)


rf_gscv


rf_gscv.fit(X_train, X_train.left.astype('int'))


print(classification_report(X_test.left.astype('int'), rf_gscv.predict(X_test.drop(columns=['left']))))


from xgboost import XGBClassifier
import numpy as np


param_xgb = {
    'model__n_estimators': [40,50,60],
    'model__max_depth':[4,6,8],
    'model__learning_rate':[.4,.2,.5,.8],
    'model__colsample_bytree':[.01,.02,.03,.4,.5,.8]
}


num_pipeline_xgb = Pipeline([('scaler',StandardScaler())])
cat_pipeline_xgb = Pipeline([('oe', OneHotEncoder())])

xgb_mdl = XGBClassifier()

preprocessing_xgb = ColumnTransformer([('num_ppln', num_pipeline_xgb, num_cols), ('cat_ppln', cat_pipeline_xgb, cat_cols)])
master_pipeline_xgb = Pipeline([('preprocessing', preprocessing_xgb), ('model', xgb_mdl)])


xgb_gscv = GridSearchCV(estimator=master_pipeline_xgb, scoring='neg_mean_squared_error', param_grid=param_xgb, n_jobs=-1, refit=True)


xgb_gscv


xgb_gscv.fit(X_train, X_train.left.astype('int'))


print(classification_report(X_test.left.astype('int'), xgb_gscv.predict(X_test.drop(columns=['left']))))








from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report, roc_auc_score, roc_curve, confusion_matrix


print("Logistic Regression roc_auc_score: " + str(roc_auc_score(X_test.left.astype('int'), logisticReg_gscv.predict(X_test.drop(columns=['left'])))))



print("Logistic Regression Confusion Matrix: \n" + str((confusion_matrix(X_test.left.astype('int'), logisticReg_gscv.predict(X_test.drop(columns=['left'])), normalize='true'))))


fpr, tpr, thresholds = roc_curve(X_test.left.astype('int'), logisticReg_gscv.predict_proba(X_test)[:,0], pos_label = 0)
plt.plot(fpr, tpr)
plt.title('ROC for logistic regression')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.show()


print("Random Forest roc_auc_score: " + str(roc_auc_score(X_test.left.astype('int'), rf_gscv.predict(X_test.drop(columns=['left'])))))


print("Random Forest Confusion Matrix: \n" + str((confusion_matrix(X_test.left.astype('int'), rf_gscv.predict(X_test.drop(columns=['left'])), normalize='true'))))


fpr, tpr, thresholds = roc_curve(X_test.left.astype('int'), rf_gscv.predict_proba(X_test)[:,0], pos_label = 0)
plt.plot(fpr, tpr)
plt.title('ROC for random forest')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.show()


print("XGBoost roc_auc_score: " + str(roc_auc_score(X_test.left.astype('int'), xgb_gscv.predict(X_test.drop(columns=['left'])))))


print("Random Forest Confusion Matrix: \n" + str((confusion_matrix(X_test.left.astype('int'), xgb_gscv.predict(X_test.drop(columns=['left'])), normalize='true'))))


fpr, tpr, thresholds = roc_curve(X_test.left.astype('int'), xgb_gscv.predict_proba(X_test)[:,0], pos_label = 0)
plt.plot(fpr, tpr)
plt.title('ROC for random forest')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.show()



